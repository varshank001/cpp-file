import pickle
import numpy as np


with open("train_qa.txt","rb") as fp:
    train_data = pickle.load(fp)

train_data



with open("test_qa.txt","rb") as fp:
    test_data = pickle.load(fp)

test_data
type(test_data)
len(test_data)
len(train_data)
train_data[0]
' '.join(train_data[0][0])
' '.join(train_data[0][1])
train_data[0][2]
vocab=set()
all_data=test_data+train_data
type(all_data)
all_data




for story,qyestion,answer in all_data:
    vocab = vocab.union(set(story))
    vocab = vocab.union(set(question))
    
    

vocab.add('yes')
vocab.add('no')
vocab

len(vocab)

vocab_len = len(vocab)+1
for data in all_data:
    print(data)
    print(len(data[0]))
    print("\n")
max_story_len = max([len(data[0])for data in all_data])
max_story_len 

max_ques_len = max([len(data[1])for data in all_data])
max_ques_len 

vocab

from keras.preprocessing,sequence import pad_sequences
from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(filters = [])
tokenizer.fit_on_texts(vocab)
tokenizer.word_index

train_story_text = []
train_quetion_text = []
train_answers = []
     
for story,question,answer in train_data:
    train_story_text.append(story)
    train_question_text.append(question)

train_story_seq = tokenizer.texts_to_sequences(train_story_text)
len(train_story_text)
train_story_seq
def vectorize_stories(data,word,index=tokenizer.word_index,
                      max_story_len = max_story_len,max_ques_len=max_ques_len):

    x=[]
    xq=[]
    y=[]


    for story,query,answer in data:
        x = [word_index[word.lower]for word in story]
        xq = [word_index[word.lower]for word in query]
        y = np.zeros(len(word_index)+1)
        y[word_index[answer]] = 1
        x.append(x)
        xq.append(xq)
        y.append(y)
    
    
    return(pad_sequence(x,maxlen = max_story_len),
          (pad_sequence(xq,maxlen = max_ques_len),
           np.array(y)

inputs_train,queries_train,answer_train=vectorize_stories(train_data)
inputs_test,queries_test,answer_test=vectorize_stories(test_data)
inputs_train
queries_test
answers_test

tokenizer.word_index['yes']
tokenizer.word_index['no']

from keras.models import sequential , Model
from keras.layers.embeddings importEmbedding
from keras.layers import Input,Activation,Dense,Permute,Dropout,add,,dot,concatenate,LSTM

input_sequence = Input((max_story_len,))
question = Input((max_ques_len,))
input_encoder_m = sequential()
input_encoder_m.add(Embedding(input_dim = vocab_len,output_dim = 64))
input_encoder_m.add(Dropout (0.3))

input_encoder_c = sequential()
input_encoder_c.add(Embedding(input_dim = vocab_len,output_dim = max_ques_len))
input_encoder_c.add(Dropout (0.3))

question_encoder = sequential()
question_encoder.add(Embedding(input_dim = vocab_len,output_dim = 64,input_length = max_ques_len))
question_encoder.add(Dropout (0.3))

input_encoded_m = input_encoder_m(input_sequence)
input_encoded_c = input_encoder_c(input_sequence)
question_encoded = question_encoder(question)
match = dot([input_encoded_m,question_encoded],axes = (2,2))
match = Activation('softmax')(match)
response = add([match,input_encoded_c])
response = permute((2,1))(response)
answer = concatenate([response,question_encoded])
answer

answer = LSTM(32)(answer)
answer = Dropout(0.5)(answer)
answer = Activation('softmax')(answer)


model = Model([input_sequence,question],answer)
model.compile(optimizer = 'rmsprop',loss= 'categorical_crossentropy',metrics = [ 'accuracy'])
model.summary()

history = model.fit([inputs_train,queries_train],answers_train,batch_size=32,epochs=20,
                   validation_data = ([inputs_test,queries_test],answers_test))


import matplotlib.pyplot as plt
print(history.history.keys())
plt.plot(history.history['accuracy'])

plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epochs')

model.save("chatbot_model")
model.load_weights("chatbot_model")
pred_results = model.predict(([input_test,queries_test]))
test_data[0][0]


story = ''.join(word for word in test_data[12][0]
story

query = ''.join(word for word in test_data[0][6])
query
test_data[15][2]

val_max = np.argmax(pred_results[23])
for key,vlaue in tokenizer.word_index.items():
    if val == val_max:
        k = key
print("predicted answer is ",k)
print("probability of certainity",pred_results[23][val_max])























